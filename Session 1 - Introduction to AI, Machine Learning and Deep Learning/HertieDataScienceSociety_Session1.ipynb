{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "HertieDataScienceSociety_Session1.ipynb",
      "provenance": [],
      "collapsed_sections": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2jBI6oWyKrG0",
        "colab_type": "text"
      },
      "source": [
        "# Hertie Data Science Society \n",
        "\n",
        "## Session 1\n",
        "\n",
        "We will start with a simple task of training our machine learning model to learn the relationship between two numbers and then predict one number by feeding the model the other. \n",
        "\n",
        "To put it into context, let's say that those two numbers are: \n",
        "- X: the number of bedrooms in a house\n",
        "- Y: the price of that house \n",
        "\n",
        "And let's assume that there's a very simple linear relationship between these 2: Y = 50000 + 50000 * X. We know that this relationship exists (it's by our design). But our model doesn't know that. So we need teach it to learn this relationship.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P27ze_51vE3o",
        "colab_type": "text"
      },
      "source": [
        "## Import \n",
        "\n",
        "We will begin by import relevant libraries. Libraries, as explained, are collection of codes and algorithms that others have written before us to help us solve some specific problems (It's called packages in R and libraries in Python). Most mainstream numerical problems that we want to solve in data science have been encountered and efficiently solved by others before us, and so we can reuse their code and solution by using libraries. \n",
        "\n",
        "We need to import these libraries first. And then call the relevant function/algorithm that we want to use in that library when we want to use it. The more you practice and solve data problems, the more you will work with these libraries and discover that there's a selected few that are usually used for most of the tasks that you need to implement. \n",
        "\n",
        "Here are a few of the important Python libraries for data science:\n",
        "- **numpy**: low-level data manipulation tool for working with data in arrays;\n",
        "- **pandas**: high-level data manipulation tool for working with data in all kinds of formats, especially tables;\n",
        "- **matplotlib**: for basic (and a bit ugly) visualization;\n",
        "- **beautifulsoup**: for webscraping and collecting data; \n",
        "- **seaborn**: for pretty visualization;\n",
        "- **sklearn**: open-sourced libraries of machine learning algorithms;\n",
        "- **tensorflow**: open-sourced libraries for deep learning (endorsed by Google);\n",
        "- **keras**: built on top of tensorflow to make writing deep learning codes simpler and easier to understand;\n",
        "- **pytorch**: another open-sourced libraries for deep learning (endorsed by Facebook)"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GQ4BomfIKdYg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Import numpy library first to create X and Y \n",
        "import numpy as np"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3TZgAYhdzoyD",
        "colab_type": "text"
      },
      "source": [
        "## Create the data\n",
        "\n",
        "The fundamental way that machine learning algorithms learn is by training and testing. The idea is that: \n",
        "- We have a dataset with information on the independent/input/features variables (Xs) and dependent/outcome variable (Y); \n",
        "- We need to split this data into a training set and a test set; \n",
        "- The machine learning model will learn the relationship of our variables in the training set; \n",
        "- Afterwards, using the test set (data it has never seen before), we will input an X variable it has never seen and see how close it can predict the Y variable.\n",
        "- The closer its guess is to the true value of Y, the better the model is at this prediction task.\n",
        "\n",
        "For our problem, let's say that we have data on prices of 7 houses and the number of bedrooms that each of these house have. We will split this data into 2: the training set with information on the first 6 houses that we will use to train the model, and a test set with information on the last house which we will use to test how good the model is at figuring out this relationship."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Eqli8t4zzufH",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "#Create the training and test data\n",
        "x_train = np.array([1, 2, 3, 4, 5, 6], dtype = float).reshape((-1,1))\n",
        "y_train = np.array([100000, 150000, 200000, 250000, 300000, 350000], dtype = float)\n",
        "x_test = np.array([7]).reshape((-1,1))\n",
        "y_test = np.array([400000])"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HhsPKkDe1_nW",
        "colab_type": "text"
      },
      "source": [
        "## Training machine learning models   \n",
        "\n",
        "We will use the sklearn libraries to try out different types of machine learning algorithms and how good they are at this prediction business. As you can see below, the process are virtually the same for different types of algorithms: \n",
        "- 1. We call the machine learning model\n",
        "- 2. We fit that model on the training set so it can learn the relationship based on the training data \n",
        "- 3. We predict the Y variable based on the X variable in the test data.\n",
        "\n",
        "Each algorithm has its strengths and weaknesses and is good for different types of problems. There is a simple guide from sklearn on how to choose a relevant algorithm based on the use case that you have here. \n",
        "\n",
        "<img src=\"https://scikit-learn.org/stable/_static/ml_map.png\">\n",
        "\n",
        "We will test out a few algorithm: \n",
        "- Linear Regression;\n",
        "- Decision Tree;\n",
        "- Random Forest;\n",
        "- XGBoost.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LnEgkmmeKpiO",
        "colab_type": "code",
        "outputId": "f8400865-3b7f-4179-e856-6d688a86efd6",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Linear Regression Model\n",
        "\n",
        "from sklearn.linear_model import LinearRegression\n",
        "\n",
        "linear_model = LinearRegression()\n",
        "linear_model.fit(x_train, y_train)\n",
        "linear_model.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([400000.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 4
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "DjTmCZ6-LKv3",
        "colab_type": "code",
        "outputId": "64a89979-8ad3-4027-8458-9cb1489491ee",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Decision Tree Regression Model \n",
        "\n",
        "\n",
        "from sklearn.tree import DecisionTreeRegressor\n",
        "\n",
        "tree_model = DecisionTreeRegressor()\n",
        "tree_model.fit(x_train, y_train)\n",
        "tree_model.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([350000.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 5
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "z22AjP7WLNyw",
        "colab_type": "code",
        "outputId": "84a624b3-dab0-4e26-cb67-78a4be50fb4a",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 88
        }
      },
      "source": [
        "# Random Forest Regression Model\n",
        "\n",
        "from sklearn.ensemble import RandomForestRegressor\n",
        "\n",
        "forest_model = RandomForestRegressor()\n",
        "forest_model.fit(x_train, y_train)\n",
        "forest_model.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/sklearn/ensemble/forest.py:245: FutureWarning: The default value of n_estimators will change from 10 in version 0.20 to 100 in 0.22.\n",
            "  \"10 in version 0.20 to 100 in 0.22.\", FutureWarning)\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([345000.])"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 7
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "5LnbkrPFLQTZ",
        "colab_type": "code",
        "outputId": "8f108636-4b41-4ee0-af69-297849bd0c01",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "# XGBoost Regression Model \n",
        "\n",
        "from xgboost import XGBRegressor\n",
        "\n",
        "xgb_model = XGBRegressor()\n",
        "xgb_model.fit(x_train, y_train)\n",
        "xgb_model.predict(x_test)"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "[07:46:38] WARNING: /workspace/src/objective/regression_obj.cu:152: reg:linear is now deprecated in favor of reg:squarederror.\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "array([348124.44], dtype=float32)"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 8
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-LoI5avVThT5",
        "colab_type": "text"
      },
      "source": [
        "## Evaluate the model\n",
        "\n",
        "As we design this relationship to be linear, the Linear Regression model unsurprisingly performs the best. This only means that Linear Regression is a good model for this particular use case. For other, more complicated use case, the other models are most likely to perform better. But Linear Regression is a good baseline upon which you can use to see how you can improve further with other models. \n",
        "\n",
        "The other models doesn't predict correctly the price for a couple of reasons: \n",
        "- There are too few training data: the more training data you have, the better the algorithm is at predicting;\n",
        "- Due to their built-in mathematical operation, most of these algorithms work in terms of probabilities. They calculated that there is a very high probability that the relationship between X and Y is somewhere around 50000 + 50000 * X, but with only 6 data points we can't know for sure. As a result, the result for price of a house with 7 bedrooms is close to 400000, but not exactly 400000."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ha64kOCP6VcO",
        "colab_type": "text"
      },
      "source": [
        "## Build a Deep Learning Model \n",
        "\n",
        "As mentioned in the workshop, deep learning is essentially trying to model the human brain neuron in its thinking. So we will try building a very simple deep learning model with just one single neuron that will try to learn the relationship between X and Y. Here are some explanation for the code: \n",
        "\n",
        "- **Dense**: a regular layer of neurons in a neural network; our network is simple so we need only 1 Dense layer; if we have a more complicated relationship that we need to model (more information on size of houses, how many stories the house has, which district it is in, is there any good schools around), then we can add more Dense layer to help the neural network learn better;\n",
        "- **units**: the number of neuron in your neural network, as the relationship is very simple, we only need 1 neuron, but if it's more complicated, we will need more; \n",
        "- **optimizer**: think of the optimizer as a learner that is sitting in that one neuron that is trying to make a guess about the relationship and the rule book that the optimizer uses to guess is the loss. There are different types of learners and each of them are good for different types of problems. For this problem, our learn is 'sgd' which is short for STOCHASTIC GRADIENT DESCENT;\n",
        "- **loss**: measures the guessed answers against the known correct answers and measures how well or how badly the optimizer did. The goal is to minimize the loss after every round of guessing to that we can try to get as close to the right value as possible; there are different rules that we can use to measure how close the guess is to the true answer, for this model, we use MEAN SQUARED ERROR;\n",
        "- **epochs**: the number of guessing rounds that the optimizer will try; you can play around with this, increase and decrease the number to 200 - 1000 - 10000, and see how the model performs. \n",
        "\n",
        "Over time, with more practice, we will learn the different and appropriate loss and optimizer functions for different scenarios."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "iYLtLnJ2TgAJ",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "# Deep Learning Model\n",
        "\n",
        "#import libraries \n",
        "import tensorflow as tf\n",
        "from tensorflow import keras\n",
        "\n",
        "model = tf.keras.Sequential([keras.layers.Dense(units =1, input_shape = [1])])\n",
        "model.compile(optimizer = 'sgd', loss = \"mean_squared_error\")\n",
        "model.fit(x_train,y_train, epochs = 500)\n",
        "print(model.predict(x_test))"
      ],
      "execution_count": 0,
      "outputs": []
    }
  ]
}